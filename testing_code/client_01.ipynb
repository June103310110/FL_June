{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import clone_model\n",
    "from model_utils import simplecnn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class clint_k():\n",
    "    def __init__(self, name, dataset, model, optimizer=None):\n",
    "        self.name = name\n",
    "        self.dataset = dataset\n",
    "        self.attri = {'name':self.name, 'dataset':self.dataset}\n",
    "        self.model = model \n",
    "    def set_params(self, weights): # input weight\n",
    "        self.model.set_weights(weights)\n",
    "        \n",
    "    def local_update(self, optimizer =  tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "                     loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                     epochs = 1\n",
    "                    ):\n",
    "#         assert model!=None, 'please use set_params method first'\n",
    "        optimizer = optimizer\n",
    "        loss_fn = loss_fn\n",
    "        model = self.model\n",
    "        model.compile(optimizer, loss_fn)\n",
    "        return model.fit(self.dataset, epochs=epochs)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CustomModel(tf.keras.Model):\n",
    "#     def train_step(self, data):\n",
    "#         # Unpack the data. Its structure depends on your model and\n",
    "#         # on what you pass to `fit()`.\n",
    "#         x, y = data\n",
    "\n",
    "#         with tf.GradientTape() as tape:\n",
    "#             y_pred = self(x, training=True)  # Forward pass\n",
    "#             # Compute the loss value\n",
    "#             # (the loss function is configured in `compile()`)\n",
    "#             loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n",
    "\n",
    "#         # Compute gradients\n",
    "#         trainable_vars = self.trainable_variables\n",
    "#         gradients = tape.gradient(loss, trainable_vars)\n",
    "#         # Update weights\n",
    "#         self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "#         # Update metrics (includes the metric that tracks the loss)\n",
    "#         self.compiled_metrics.update_state(y, y_pred)\n",
    "#         # Return a dict mapping metric names to current value\n",
    "        \n",
    "#         self.get_grads = [g.numpy() for g in gradients]\n",
    "#         return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "# z.model.get_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.keras.engine.functional.Functional"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = simplecnn()\n",
    "type(model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = CustomModel(model.inputs, model.outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(X_train, y_train), (X_test_origin, y_test_origin) = mnist.load_data()\n",
    "\n",
    "X_train = X_train/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).cache().batch(64).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.functional.Functional at 0x7f94b415f0d0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = clint_k(name='id0', dataset=train_dataset, model=model)\n",
    "z.set_params(model.get_weights())\n",
    "z.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# z.local_update()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " ### Creating a custom optimizer\n",
    "  If you intend to create your own optimization algorithm, simply inherit from\n",
    "  this class and override the following methods:\n",
    "    - `_resource_apply_dense` (update variable given gradient tensor is a dense\n",
    "      `tf.Tensor`)\n",
    "    - `_resource_apply_sparse` (update variable given gradient tensor is a\n",
    "      sparse `tf.IndexedSlices`. The most common way for this to happen\n",
    "      is if you are taking the gradient through a `tf.gather`.)\n",
    "    - `_create_slots`\n",
    "      (if your optimizer algorithm requires additional variables)\n",
    "    - `get_config`\n",
    "      (serialization of the optimizer, include all hyper parameters)\n",
    "  \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerturbedGradientDescent(tf.keras.optimizers.SGD):\n",
    "    def __init__(self,\n",
    "              name=\"PGD\",\n",
    "              mu=0.01,\n",
    "               **kwargs):\n",
    "        super().__init__(name = name, **kwargs)\n",
    "#         print(name)\n",
    "#         print(self._momentum)\n",
    "#         print(self.nesterov)\n",
    "        self.mu = mu\n",
    "        self._set_hyper(\"prox_mu\", self.mu)\n",
    "        # Tensor versions of the constructor arguments, created in _prepare().\n",
    "        self._lr_t = None\n",
    "        self._mu_t = None\n",
    "        \n",
    "        self._lr_t = tf.convert_to_tensor(self._hyper['learning_rate'], name=\"learning_rate\")\n",
    "        self._mu_t = tf.convert_to_tensor(self.mu, name=\"prox_mu\")\n",
    "#     def _prepare_local(self, **kwargs):\n",
    "        \n",
    "    def _create_slots(self, var_list):\n",
    "#         super()._create_slots(var_device, var_dtype, apply_state)\n",
    "        \n",
    "        for var in var_list:\n",
    "            self.add_slot(var, \"vstar\")\n",
    "#             super().__init__(**kwargs)\n",
    "    \n",
    "    def _resource_apply_dense(self, grad, var):\n",
    "        lr_t = tf.cast(self._lr_t, var.dtype.base_dtype)\n",
    "        mu_t = tf.cast(self._mu_t, var.dtype.base_dtype)\n",
    "        vstar = self.get_slot(var, \"vstar\")\n",
    "        \n",
    "        var_update = var.assign_sub(lr_t*(grad + mu_t*(var-vstar)))\n",
    "\n",
    "#         return tf.group(*[var_update,])\n",
    "        return tf.raw_ops.ResourceApplyGradientDescent(\n",
    "          var=var_update.handle,\n",
    "          alpha=lr_t,\n",
    "          delta=grad,\n",
    "          use_locking=self._use_locking)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"learning_rate\": self._serialize_hyperparameter(\"learning_rate\"),\n",
    "            \"decay\": self._initial_decay,\n",
    "            \"momentum\": self._serialize_hyperparameter(\"momentum\"),\n",
    "            \"nesterov\": self.nesterov,\n",
    "            \"prox_mu\": self.mu,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'PGD',\n",
       " 'learning_rate': 0.01,\n",
       " 'decay': 0.0,\n",
       " 'momentum': 0.0,\n",
       " 'nesterov': True,\n",
       " 'prox_mu': 0.01}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = PerturbedGradientDescent(nesterov=True)\n",
    "a.get_config()\n",
    "# a._hyper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 3s 3ms/step - loss: 2.2419\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f94b4d64a90>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = clint_k(name='id0', dataset=train_dataset, model=model)\n",
    "z.set_params(model.get_weights())\n",
    "\n",
    "optimizer = PerturbedGradientDescent()\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    \n",
    "    \n",
    "z.local_update(optimizer = optimizer, loss_fn=loss_fn, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "class Employee:\n",
    "    def __init__(self):\n",
    "        self.cut_tree = 3\n",
    "\n",
    "class Andy(Employee):\n",
    "    def __init__(self, get_gold):\n",
    "        super().__init__()\n",
    "        self.get_gold = get_gold\n",
    "\n",
    "    def getDetials(self):\n",
    "        print('==getDetals==')\n",
    "        print('tree:', self.cut_tree)\n",
    "        print('gold:', self.get_gold)\n",
    "        \n",
    "andy = Andy(10)\n",
    "andy.getDetials()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
