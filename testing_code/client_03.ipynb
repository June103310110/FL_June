{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import clone_model\n",
    "from model_utils import simplecnn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class clint_k():\n",
    "    def __init__(self, name, dataset, model, \n",
    "                optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "                loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                ):\n",
    "        self.name = name\n",
    "        self.dataset = dataset\n",
    "        self.attri = {'name':self.name, 'dataset':self.dataset}\n",
    "        self.model = model \n",
    "        self.optimizer = optimizer\n",
    "        self.model.compile(self.optimizer, loss_fn)\n",
    "        self.warmingUp = False\n",
    "        self.loss_fn = loss_fn\n",
    "    @tf.function\n",
    "    def set_params(self, weights): # input weight\n",
    "        self.model.set_weights(weights)\n",
    "        \n",
    "#     @tf.function\n",
    "    def warming_up(self):\n",
    "        print('Warming up process, setting slot from trainable weight')\n",
    "        layer_trainable = {}\n",
    "        model = self.model\n",
    "        for l in model.layers:\n",
    "            layer_trainable[l.name] = l.trainable\n",
    "            l.trainable = False\n",
    "            \n",
    "        model.fit(self.dataset, verbose = False)\n",
    "\n",
    "        for l in layer_trainable.keys():\n",
    "            model.get_layer(l).trainable = True\n",
    "#     def warming_up(self):\n",
    "\n",
    "        # =================\n",
    "#         for step, (x_batch_train, y_batch_train) in enumerate(self.dataset):\n",
    "\n",
    "#             # Open a GradientTape to record the operations run\n",
    "#             # during the forward pass, which enables auto-differentiation.\n",
    "#             with tf.GradientTape() as tape:\n",
    "\n",
    "#                 # Run the forward pass of the layer.\n",
    "#                 # The operations that the layer applies\n",
    "#                 # to its inputs are going to be recorded\n",
    "#                 # on the GradientTape.\n",
    "#                 logits = self.model(x_batch_train, training=True)  # Logits for this minibatch\n",
    "\n",
    "#                 # Compute the loss value for this minibatch.\n",
    "#                 loss_value = self.loss_fn(y_batch_train, logits)\n",
    "\n",
    "#             # Use the gradient tape to automatically retrieve\n",
    "#             # the gradients of the trainable variables with respect to the loss.\n",
    "#             grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "\n",
    "            # Run one step of gradient descent by updating\n",
    "            # the value of the variables to minimize the loss.\n",
    "#             self.optimizer.apply_gradients(zip(grads, self.model.trainable_weights))\n",
    "\n",
    "        # ============\n",
    "#     def warming_up(self):\n",
    "#         model = self.model\n",
    "#         model([x for (x,y) in self.dataset.take(1)][0], training=True)\n",
    "\n",
    "    def local_update(self,\n",
    "                    epochs = 1\n",
    "                    ):\n",
    "        if not self.warmingUp:\n",
    "            self.warming_up()\n",
    "            self.optimizer.set_params(self.model)\n",
    "            self.warmingUp = True\n",
    "            print('self.warmingUp finish')\n",
    "        else:\n",
    "            self.optimizer.set_params(self.model)\n",
    "        return model.fit(self.dataset, epochs=epochs)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerturbedGradientDescent(tf.keras.optimizers.SGD):\n",
    "    def __init__(self,\n",
    "              name=\"PGD\",\n",
    "              mu=0.01,\n",
    "               **kwargs):\n",
    "        super().__init__(name = name, **kwargs)\n",
    "#         print(name)\n",
    "\n",
    "        self._set_hyper(\"prox_mu\", mu)\n",
    "        # Tensor versions of the constructor arguments, created in _prepare().\n",
    "        self._lr_t = None\n",
    "        self._mu_t = None\n",
    "        self.vstar = None\n",
    "        \n",
    "        self._lr_t = tf.convert_to_tensor(self._hyper['learning_rate'], name=\"learning_rate\")\n",
    "        self._mu_t = tf.convert_to_tensor(self._hyper['prox_mu'], name=\"prox_mu\")\n",
    "        \n",
    "    def _create_slots(self, var_list):\n",
    "\n",
    "        for var in var_list:\n",
    "            self.add_slot(var, \"vt\")\n",
    "\n",
    "    \n",
    "    def _resource_apply_dense(self, grad, var):\n",
    "        lr_t = tf.cast(self._lr_t, var.dtype.base_dtype)\n",
    "        mu_t = tf.cast(self._mu_t, var.dtype.base_dtype)\n",
    "        \n",
    "        vp = self.get_slot(var, \"vt\") # mv = this client's traiable variable \n",
    "        \n",
    "        var_update = var.assign_sub(lr_t*(grad + mu_t*(var-vp)))\n",
    "\n",
    "        return tf.raw_ops.ResourceApplyGradientDescent(\n",
    "          var=var_update.handle,\n",
    "          alpha=lr_t,\n",
    "          delta=grad,\n",
    "          use_locking=self._use_locking)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"learning_rate\": self._serialize_hyperparameter(\"learning_rate\"),\n",
    "            \"decay\": self._initial_decay,\n",
    "            \"momentum\": self._serialize_hyperparameter(\"momentum\"),\n",
    "            \"nesterov\": self.nesterov,\n",
    "            \"prox_mu\": self.mu,\n",
    "        })\n",
    "        return config\n",
    "    \n",
    "    def set_params(self, model):\n",
    "        for weights in model.trainable_variables:\n",
    "            vp = self.get_slot(weights, \"vt\") \n",
    "            vp.assign(weights)\n",
    "#             print(vp)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.keras.engine.functional.Functional"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = simplecnn('server')\n",
    "type(model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(X_train, y_train), (X_test_origin, y_test_origin) = mnist.load_data()\n",
    "\n",
    "X_train = X_train/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).cache().batch(64).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warming up process, setting slot from trainable weight\n",
      "self.warmingUp finish\n",
      "Epoch 1/3\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.7246\n",
      "Epoch 2/3\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.6938\n",
      "Epoch 3/3\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.6827\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f96cc09ae10>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "z = clint_k(name='id0', dataset=train_dataset, model=model,\n",
    "           optimizer = PerturbedGradientDescent())\n",
    "tf.keras.backend.clear_session()\n",
    "z.local_update(epochs = 3)\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
